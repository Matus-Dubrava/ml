{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# chaining transformations\n",
    "dataset = dataset.repeat(3).batch(7) # repeat the whole dataset 3x and produces batches of 7 items\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# map function is applied to every item\n",
    "dataset = dataset.map(lambda x: x * 2)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# apply function is applied to the whole dataset\n",
    "dataset = dataset.apply(tf.data.experimental.unbatch())\n",
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# filtering the dataset\n",
    "dataset = dataset.filter(lambda x: x < 10)\n",
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# shuffling dataset (using beffer)\n",
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data from multiple sources while shuffling them\n",
    "train_filepaths = ['data/housing/my_train_*.csv']\n",
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\n",
    "\n",
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'3.6471,19.0,6.479338842975206,0.9628099173553719,1631.0,3.3698347107438016,37.32,-120.45,1.014', shape=(), dtype=string)\n",
      "tf.Tensor(b'3.6471,19.0,6.479338842975206,0.9628099173553719,1631.0,3.3698347107438016,37.32,-120.45,1.014', shape=(), dtype=string)\n",
      "tf.Tensor(b'3.6471,19.0,6.479338842975206,0.9628099173553719,1631.0,3.3698347107438016,37.32,-120.45,1.014', shape=(), dtype=string)\n",
      "tf.Tensor(b'3.6471,19.0,6.479338842975206,0.9628099173553719,1631.0,3.3698347107438016,37.32,-120.45,1.014', shape=(), dtype=string)\n",
      "tf.Tensor(b'3.6471,19.0,6.479338842975206,0.9628099173553719,1631.0,3.3698347107438016,37.32,-120.45,1.014', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(5):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.87614072e+00  2.86598191e+01  5.43676547e+00  1.09636817e+00\n",
      "  1.43038979e+03  3.07711721e+00  3.56146835e+01 -1.19552319e+02] [1.89840947e+00 1.26200248e+01 2.56781628e+00 4.90943971e-01\n",
      " 1.13369286e+03 1.11416114e+01 2.13471721e+00 2.00299702e+00]\n"
     ]
    }
   ],
   "source": [
    "# assume that we have mean, and std for X \n",
    "from sklearn.datasets import california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = california_housing.fetch_california_housing()\n",
    "X = housing['data']\n",
    "y = housing['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_\n",
    "print(X_mean, X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([-0.12064882, -0.7654358 ,  0.40601543, -0.27204382,  0.17695288,\n",
       "         0.02627245,  0.79884803, -0.4481657 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.014], dtype=float32)>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inputs = 8\n",
    "\n",
    "def preprocess(line):\n",
    "    # here we have 8 input features with default, default value for each of them is 0.\n",
    "    # then we have 1 target with default without default value => if value is missing, exception \n",
    "    # will be raised\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean) / X_std, y\n",
    "\n",
    "preprocessing(b'3.6471,19.0,6.479338842975206,0.9628099173553719,1631.0,3.3698347107438016,37.32,-120.45,1.014')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5, \n",
    "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                       n_parse_threads=5, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths = ['data/housing/my_train_*.csv']\n",
    "valid_filepaths = ['data/housing/my_valid.csv']\n",
    "test_filepaths = ['data/housing/my_test.csv']\n",
    "\n",
    "train_set = csv_reader_dataset(train_filepaths)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "467/467 [==============================] - 4s 9ms/step - loss: 1.2043 - val_loss: 3.9460\n",
      "Epoch 2/1000\n",
      "467/467 [==============================] - 2s 4ms/step - loss: 0.4320 - val_loss: 2.6058\n",
      "Epoch 3/1000\n",
      "467/467 [==============================] - 2s 4ms/step - loss: 0.4100 - val_loss: 1.4347\n",
      "Epoch 4/1000\n",
      "467/467 [==============================] - 2s 5ms/step - loss: 0.3881 - val_loss: 0.5806\n",
      "Epoch 5/1000\n",
      "467/467 [==============================] - 2s 5ms/step - loss: 0.3725 - val_loss: 0.4111: 0s - los\n",
      "Epoch 6/1000\n",
      "467/467 [==============================] - 2s 5ms/step - loss: 0.3645 - val_loss: 0.5638\n",
      "Epoch 7/1000\n",
      "467/467 [==============================] - 2s 5ms/step - loss: 0.3548 - val_loss: 0.6163\n",
      "Epoch 8/1000\n",
      "467/467 [==============================] - 2s 5ms/step - loss: 0.3449 - val_loss: 0.6201\n",
      "Epoch 9/1000\n",
      "467/467 [==============================] - 2s 5ms/step - loss: 0.3381 - val_loss: 0.5652\n",
      "Epoch 10/1000\n",
      "467/467 [==============================] - 2s 5ms/step - loss: 0.3410 - val_loss: 0.4341\n",
      "Epoch 11/1000\n",
      "467/467 [==============================] - 2s 5ms/step - loss: 0.3369 - val_loss: 0.4112\n",
      "Epoch 12/1000\n",
      "467/467 [==============================] - 2s 5ms/step - loss: 0.3417 - val_loss: 0.3668\n",
      "Epoch 13/1000\n",
      "467/467 [==============================] - 2s 5ms/step - loss: 0.3277 - val_loss: 0.3609\n",
      "Epoch 14/1000\n",
      "467/467 [==============================] - 2s 5ms/step - loss: 0.3244 - val_loss: 0.3712\n",
      "Epoch 15/1000\n",
      "467/467 [==============================] - 2s 5ms/step - loss: 0.3217 - val_loss: 0.3768\n",
      "Epoch 16/1000\n",
      "467/467 [==============================] - 2s 5ms/step - loss: 0.3178 - val_loss: 0.4961\n",
      "Epoch 17/1000\n",
      "467/467 [==============================] - 2s 5ms/step - loss: 0.3234 - val_loss: 0.4354\n",
      "Epoch 18/1000\n",
      "467/467 [==============================] - 2s 5ms/step - loss: 0.3161 - val_loss: 0.5081\n",
      "Epoch 19/1000\n",
      "467/467 [==============================] - 2s 5ms/step - loss: 0.3188 - val_loss: 0.4761\n",
      "Epoch 20/1000\n",
      "467/467 [==============================] - 2s 5ms/step - loss: 0.3121 - val_loss: 0.4249\n",
      "Epoch 21/1000\n",
      "467/467 [==============================] - 2s 5ms/step - loss: 0.3126 - val_loss: 0.4977\n",
      "Epoch 22/1000\n",
      "467/467 [==============================] - 2s 5ms/step - loss: 0.3112 - val_loss: 0.4872\n",
      "Epoch 23/1000\n",
      "467/467 [==============================] - 2s 5ms/step - loss: 0.3094 - val_loss: 0.7181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13d672ac8>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(30))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(activation='elu'))\n",
    "model.add(keras.layers.Dense(30))\n",
    "model.add(keras.layers.Activation(activation='elu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(1))\n",
    "model.compile(loss='mean_squared_error')\n",
    "model.fit(train_set, epochs=1000, validation_data=valid_set, \n",
    "          callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     97/Unknown - 1s 6ms/step - loss: 0.3617"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.36165607574674274"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3529316 ],\n",
       "       [1.2954531 ],\n",
       "       [3.1000936 ],\n",
       "       [2.3362093 ],\n",
       "       [1.7327207 ],\n",
       "       [1.5806928 ],\n",
       "       [1.5339705 ],\n",
       "       [1.5658405 ],\n",
       "       [1.7004864 ],\n",
       "       [1.3354988 ],\n",
       "       [1.2801796 ],\n",
       "       [4.461221  ],\n",
       "       [2.2197528 ],\n",
       "       [2.3680706 ],\n",
       "       [1.8634199 ],\n",
       "       [1.9733782 ],\n",
       "       [1.5613539 ],\n",
       "       [3.0820854 ],\n",
       "       [1.2606753 ],\n",
       "       [2.427564  ],\n",
       "       [0.29964465],\n",
       "       [2.3101547 ],\n",
       "       [3.324477  ],\n",
       "       [2.7751498 ],\n",
       "       [2.2630048 ],\n",
       "       [1.863718  ],\n",
       "       [2.4575977 ],\n",
       "       [1.3613247 ],\n",
       "       [3.9319975 ],\n",
       "       [2.5511432 ],\n",
       "       [1.1018442 ],\n",
       "       [3.5492716 ],\n",
       "       [1.2775346 ],\n",
       "       [0.68823963],\n",
       "       [1.7904631 ],\n",
       "       [1.7566411 ],\n",
       "       [0.8878375 ],\n",
       "       [1.5761279 ],\n",
       "       [2.2615128 ],\n",
       "       [2.7767522 ],\n",
       "       [1.4034616 ],\n",
       "       [3.625371  ],\n",
       "       [2.4840877 ],\n",
       "       [1.5432407 ],\n",
       "       [1.492744  ],\n",
       "       [1.5696832 ],\n",
       "       [1.2224617 ],\n",
       "       [1.7437705 ],\n",
       "       [1.6064442 ],\n",
       "       [1.2202785 ],\n",
       "       [0.9078443 ],\n",
       "       [3.4720538 ],\n",
       "       [2.6109126 ],\n",
       "       [1.923635  ],\n",
       "       [1.6163493 ],\n",
       "       [1.1671276 ],\n",
       "       [3.1421587 ],\n",
       "       [1.3231752 ],\n",
       "       [2.274763  ],\n",
       "       [1.8158973 ],\n",
       "       [2.173851  ],\n",
       "       [1.4425354 ],\n",
       "       [0.65471536],\n",
       "       [1.5517638 ],\n",
       "       [0.701374  ],\n",
       "       [1.6547133 ],\n",
       "       [0.78832173],\n",
       "       [2.487619  ],\n",
       "       [1.9399732 ],\n",
       "       [2.2903445 ],\n",
       "       [3.3167574 ],\n",
       "       [2.3962893 ],\n",
       "       [1.3336347 ],\n",
       "       [1.3882228 ],\n",
       "       [0.9666866 ],\n",
       "       [1.3674923 ],\n",
       "       [2.0324657 ],\n",
       "       [2.5550582 ],\n",
       "       [0.6564694 ],\n",
       "       [2.3617628 ],\n",
       "       [1.5854903 ],\n",
       "       [0.98286194],\n",
       "       [2.2492416 ],\n",
       "       [3.2346792 ],\n",
       "       [2.9009588 ],\n",
       "       [2.1641946 ],\n",
       "       [0.7966199 ],\n",
       "       [2.0045404 ],\n",
       "       [2.4171813 ],\n",
       "       [3.03556   ],\n",
       "       [1.6088561 ],\n",
       "       [0.49506575],\n",
       "       [2.1685    ],\n",
       "       [3.1959841 ],\n",
       "       [2.7475092 ],\n",
       "       [0.9397612 ]], dtype=float32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_set = test_set.take(3).map(lambda X, y: X)\n",
    "model.predict(new_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFRecord format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# creating TFRecord format with compression\n",
    "\n",
    "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\", options) as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")\n",
    "    \n",
    "filepaths = [\"my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths, compression_type=\"GZIP\")\n",
    "\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing MNIST dataset (protobuf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist['data'].astype(np.uint8), mnist['target'].astype(np.uint8)\n",
    "X_train, X_valid, X_test = X[:50000], X[50000:60000], X[60000:]\n",
    "y_train, y_valid, y_test = y[:50000], y[50000:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _image_to_protobuf(image, label):\n",
    "    '''Transform one MNIST image + label into protobuf format.'''\n",
    "    \n",
    "    return tf.train.Example(\n",
    "        features = tf.train.Features(\n",
    "            feature={\n",
    "                \"image\": tf.train.Feature(int64_list=tf.train.Int64List(value=image)),\n",
    "                \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "def images_to_protobuf(images, labels, filepaths):\n",
    "    n_obs = len(images) // len(filepaths)\n",
    "    \n",
    "    for i, filepath in enumerate(filepaths):\n",
    "        with tf.io.TFRecordWriter(filepath) as f:\n",
    "            print(f\"saving range {i*n_obs} : {(i+1)*n_obs}\")\n",
    "            for image, label in zip(images[i*n_obs:(i+1)*n_obs], labels[i*n_obs:(i+1)*n_obs]):\n",
    "                f.write(_image_to_protobuf(image, label).SerializeToString())\n",
    "            \n",
    "def load_images(filepaths):\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([784], tf.int64, default_value=[0] * 784),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=0)\n",
    "    }\n",
    "    \n",
    "    X, y = [], []\n",
    "    for serialized_image in tf.data.TFRecordDataset(filepaths):\n",
    "        parsed_image = tf.io.parse_single_example(serialized_image, feature_description)\n",
    "        X.append(parsed_image[\"image\"])\n",
    "        y.append(parsed_image[\"label\"])\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving range 0 : 5000\n",
      "saving range 5000 : 10000\n",
      "saving range 10000 : 15000\n",
      "saving range 15000 : 20000\n",
      "saving range 20000 : 25000\n",
      "saving range 25000 : 30000\n",
      "saving range 30000 : 35000\n",
      "saving range 35000 : 40000\n",
      "saving range 40000 : 45000\n",
      "saving range 45000 : 50000\n"
     ]
    }
   ],
   "source": [
    "filepaths = [f\"data/mnist/trainset_{i}.tfrecord\" for i in range(10)]\n",
    "images_to_protobuf(X_train, y_train, filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_images(filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying movie revies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tfds.load(\"imdb_reviews\", batch_size=32, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train = ratings[\"train\"].prefetch(1)\n",
    "ratings_other = ratings[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
